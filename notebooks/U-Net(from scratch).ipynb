{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as tv\n",
    "import torchvision.datasets as dset\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility blocks for building a UNet model\n",
    "\n",
    "# Convolutional Block used in downscaling \n",
    "def conv_block(inp_dim, out_dim):\n",
    "    model = nn.Sequential(\n",
    "                    nn.Conv2d(inp_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
    "                    nn.BatchNorm2d(out_dim),\n",
    "                    nn.LeakyReLU(0.2, inplace=True),\n",
    "                    nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1,padding=1),\n",
    "                    nn.BatchNorm2d(out_dim)\n",
    "            )\n",
    "    return model\n",
    "\n",
    "# Deconvolutional Block used in upscaling \n",
    "def trans_conv_block(inp_dim, out_dim):\n",
    "    model = nn.Sequential(\n",
    "                    nn.ConvTranspose2d(inp_dim, out_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                    nn.BatchNorm2d(out_dim), \n",
    "                    nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "    return model\n",
    "\n",
    "# maxpool\n",
    "def maxpool():\n",
    "    pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../imgs/unet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now putting the complete architecture all together\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, num_filter):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        self.inp_dim = inp_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_filter = num_filter\n",
    "        \n",
    "        # downscaling\n",
    "        self.down1 = conv_block(self.inp_dim, self.num_filter)\n",
    "        self.pool1 = maxpool()\n",
    "        self.down2 = conv_block(self.num_filter, self.num_filter*2)\n",
    "        self.pool2 = maxpool()\n",
    "        self.down3 = conv_block(self.num_filter*2, self.num_filter*4)\n",
    "        self.pool3 = maxpool()\n",
    "        self.down4 = conv_block(self.num_filter*4, self.num_filter*8)\n",
    "        self.pool4 = maxpool()\n",
    "        \n",
    "        self.bridge = conv_block(self.num_filter*8, self.num_filter*16)\n",
    "        \n",
    "        # upscaling\n",
    "        self.trans1 = trans_conv_block(self.num_filter*16, self.num_filter*8)\n",
    "        self.up1 = conv_block(self.num_filter*16, self.num_filter*8)\n",
    "        self.trans2 = trans_conv_block(self.num_filter*8, self.num_filter*4)\n",
    "        self.up2 = conv_block(self.num_filter*8, self.num_filter*4)\n",
    "        self.tarns3 = trans_conv_block(self.num_filter*4, self.num_filter*2)\n",
    "        self.up3 = conv_block(self.num_filter*4, self.num_filter*2)\n",
    "        self.tarns4 = trans_conv_block(self.num_filter*2, self.num_filter*1)\n",
    "        self.up4 = conv_block(self.num_filter*2, self.num_filter*1)\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "                        nn.Conv2d(self.num_filter,self.out_dim,3,1,1),\n",
    "                        nn.Tanh(),\n",
    "                        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        down1 = self.down1(input)\n",
    "        pool1 = self.pool1(self.down1)\n",
    "        down2 = self.down2(pool1)\n",
    "        pool2 = self.pool2(down2)\n",
    "        down3 = self.down3(pool2)\n",
    "        pool3 = self.pool3(down3)\n",
    "        down4 = self.down4(pool3)\n",
    "        pool4 = self.pool4(down4)\n",
    "        \n",
    "        bridge = self.bridge(pool4)\n",
    "        \n",
    "        trans1 = self.trans1(bridge)\n",
    "        concat1 = torch.cat([tarns1, down4], dim=1)\n",
    "        up1 = self.up1(concat1)\n",
    "        trans2 = self.trans2(up1)\n",
    "        concat2 = torch.cat([tarns2, down3], dim=1)\n",
    "        up2 = self.up2(concat2)\n",
    "        trans3 = self.trans3(up2)\n",
    "        concat3 = torch.cat([tarns3, down2], dim=1)\n",
    "        up3 = self.up3(concat3)\n",
    "        trans4 = self.trans4(up3)\n",
    "        concat4 = torch.cat([tarns4, down1], dim=1)\n",
    "        up4 = self.up4(concat4)\n",
    "        \n",
    "        out = self.out(up4)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(3, 3, 64)\n",
    "model = model.cuda()\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "img_size = 200\n",
    "lr = 0.0002\n",
    "epoch = 100\n",
    "# preparing dataset from folders containing images and masks\n",
    "image_data = dset.ImageFolder(root=\"drive/SemanticDataset/train/\", transform = transforms.Compose([\n",
    "                                            transforms.Scale(size=img_size),\n",
    "                                            transforms.CenterCrop(size=(img_size,img_size*2)),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            ]))\n",
    "\n",
    "label_data = dset.ImageFolder(root=\"drive/SemanticDataset/label/\", transform = transforms.Compose([\n",
    "                                            transforms.Scale(size=img_size),\n",
    "                                            transforms.CenterCrop(size=(img_size,img_size*2)),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# defining dataloaders\n",
    "image_batch = data.DataLoader(image_data, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "label_batch = data.DataLoader(label_data, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining loss functions\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "loss= []\n",
    "\n",
    "# training model\n",
    "for i in range(epoch):\n",
    "    for _, (image, label) in enumerate(zip(image_batch, label_batch)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = Variable(image, requires_grad=True).cuda()\n",
    "        y = Variable(label).cuda()\n",
    "\n",
    "        out = model.forward(x)\n",
    "        loss = loss_func(out, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if _ % 100 == 0:\n",
    "          print(\"Epoch: \"+i+\"| Loss: \" , loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
